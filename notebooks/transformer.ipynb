{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "A Jupyther notebook is used to rapidly iterate through the development of data ingestion and pre-processing, transformer model architecture and hyperparameter tuning.\n",
    "\n",
    "The MB2016 Data Module processes the raw 2016 Moonboard dataset into train, validation and test datasets ready as inputs for the transformer deep learning model.\n",
    "\n",
    "Debug MB2016 prepare_data and setup."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "from grade_predictor.data.mb2016 import MB2016\n",
    "mb2016 = MB2016()\n",
    "mb2016.prepare_data()\n",
    "mb2016.setup()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "from IPython.display import display"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import logging  # import some stdlib components to control what's display\n",
    "import textwrap\n",
    "import traceback\n",
    "from grade_predictor.lit_models.base import BaseLitModel\n",
    "\n",
    "\n",
    "class LinearRegression(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()  # just like in torch.nn.Module, we need to call the parent class __init__\n",
    "\n",
    "        # attach torch.nn.Modules as top level attributes during init, just like in a torch.nn.Module\n",
    "        n, d_model = 199, 3\n",
    "        nhead, nlayers = 1, 1\n",
    "        self.embedding = torch.nn.Embedding(n, d_model, max_norm=True)\n",
    "        encoder_layers = torch.nn.TransformerEncoderLayer(d_model, nhead)\n",
    "        self.transformer_encoder = torch.nn.TransformerEncoder(encoder_layers, nlayers)\n",
    "        # self.position_embedding = torch.nn.Embedding(n, d, max_norm=True)\n",
    "        self.linear = torch.nn.Linear(in_features=d_model, out_features=1)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.linear2 = torch.nn.Linear(in_features=15, out_features=1)\n",
    "        # we like to define the entire model as one torch.nn.Module -- typically in a separate class\n",
    "\n",
    "\n",
    "    # optionally, define a forward method\n",
    "    def forward(self, xs):\n",
    "        # xs = xs[0]\n",
    "        # print(xs.size())\n",
    "        # display(xs)\n",
    "        # print(xs.type())\n",
    "        xs = self.embedding(xs)\n",
    "        xs = self.transformer_encoder(xs)  # we like to just call the model's forward method\n",
    "        xs = self.linear(xs)\n",
    "        xs = self.flatten(xs)\n",
    "        xs = self.linear2(xs)\n",
    "        return xs\n",
    "\n",
    "def training_step(self: pl.LightningModule, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
    "    xs, ys = batch  # unpack the batch\n",
    "    xs = xs[:,0]\n",
    "    ys = ys.unsqueeze(1)\n",
    "    outs = self(xs)  # apply the model\n",
    "    loss = torch.nn.functional.mse_loss(outs, ys)  # compute the (squared error) loss\n",
    "    return loss\n",
    "\n",
    "\n",
    "def configure_optimizers(self: LinearRegression) -> torch.optim.Optimizer:\n",
    "    optimizer = torch.optim.Adam(self.parameters(), lr=3e-4)  # https://fsdl.me/ol-reliable-img\n",
    "    return optimizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================================================\n",
      "Layer (type:depth-idx)                        Output Shape              Param #\n",
      "===============================================================================================\n",
      "├─Embedding: 1-1                              [-1, 15, 3]               597\n",
      "├─TransformerEncoder: 1-2                     [-1, 15, 3]               --\n",
      "|    └─ModuleList: 2                          []                        --\n",
      "|    |    └─TransformerEncoderLayer: 3-1      [-1, 15, 3]               14,399\n",
      "├─Linear: 1-3                                 [-1, 15, 1]               4\n",
      "├─Flatten: 1-4                                [-1, 15]                  --\n",
      "├─Linear: 1-5                                 [-1, 1]                   16\n",
      "===============================================================================================\n",
      "Total params: 15,016\n",
      "Trainable params: 15,016\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.04\n",
      "===============================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.24\n",
      "Params size (MB): 0.06\n",
      "Estimated Total Size (MB): 0.29\n",
      "===============================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "===============================================================================================\nLayer (type:depth-idx)                        Output Shape              Param #\n===============================================================================================\n├─Embedding: 1-1                              [-1, 15, 3]               597\n├─TransformerEncoder: 1-2                     [-1, 15, 3]               --\n|    └─ModuleList: 2                          []                        --\n|    |    └─TransformerEncoderLayer: 3-1      [-1, 15, 3]               14,399\n├─Linear: 1-3                                 [-1, 15, 1]               4\n├─Flatten: 1-4                                [-1, 15]                  --\n├─Linear: 1-5                                 [-1, 1]                   16\n===============================================================================================\nTotal params: 15,016\nTrainable params: 15,016\nNon-trainable params: 0\nTotal mult-adds (M): 0.04\n===============================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.24\nParams size (MB): 0.06\nEstimated Total Size (MB): 0.29\n==============================================================================================="
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "model = LinearRegression()\n",
    "summary(model, (15,), dtypes=[torch.int32])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | embedding           | Embedding          | 597   \n",
      "1 | transformer_encoder | TransformerEncoder | 14.4 K\n",
      "2 | linear              | Linear             | 4     \n",
      "3 | flatten             | Flatten            | 0     \n",
      "4 | linear2             | Linear             | 16    \n",
      "-----------------------------------------------------------\n",
      "15.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.0 K    Total params\n",
      "0.060     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f8047ee11e01416ca7ae25fa029e479d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x117954310>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/henry/anaconda3/envs/moonboard-transformer-ml/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/Users/henry/anaconda3/envs/moonboard-transformer-ml/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1442, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/Users/henry/anaconda3/envs/moonboard-transformer-ml/lib/python3.10/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/Users/henry/anaconda3/envs/moonboard-transformer-ml/lib/python3.10/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/Users/henry/anaconda3/envs/moonboard-transformer-ml/lib/python3.10/multiprocessing/connection.py\", line 931, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/Users/henry/anaconda3/envs/moonboard-transformer-ml/lib/python3.10/selectors.py\", line 416, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\n",
    "\n",
    "LinearRegression.configure_optimizers = configure_optimizers\n",
    "LinearRegression.training_step = training_step\n",
    "\n",
    "trainer = pl.Trainer(gpus=int(torch.cuda.is_available()), max_epochs=10)\n",
    "trainer.fit(model=model, train_dataloaders=mb2016.train_dataloader())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}