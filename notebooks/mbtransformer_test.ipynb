{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "The following code sets up the environment used by FSDL for the labs used in their 2022 deep learning course."
   ],
   "metadata": {
    "id": "uGxm5GZt-xEa",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"Sets up both local Jupyter and Google Colab notebooks in the same state.\"\"\"\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import sys\n",
    "import subprocess\n",
    "from subprocess import PIPE, Popen\n",
    "\n",
    "try:  # check if we're in a git repo\n",
    "    repo_dir = subprocess.run([\"git\", \"rev-parse\", \"--show-toplevel\"], capture_output=True, check=True).stdout.decode().strip()\n",
    "    repo = Path(repo_dir).name\n",
    "except subprocess.CalledProcessError:\n",
    "    repo = os.environ.get(\"MBML_REPO\", \"moonboard-transformer-ml\")\n",
    "\n",
    "branch = os.environ.get(\"MBML_BRANCH\", \"dev\")\n",
    "token = os.environ.get(\"MBML_GHTOKEN\")\n",
    "prefix = token + \"@\" if token else \"\"\n",
    "\n",
    "in_colab = \"google.colab\" in sys.modules\n",
    "\n",
    "def _go():\n",
    "    if in_colab: # create the repo and cd into it\n",
    "        repo_root = Path(\"/\") / \"content\" / repo\n",
    "        os.chdir(repo_root.parent)\n",
    "\n",
    "        shutil.rmtree(repo_root, ignore_errors=True)\n",
    "        _clone_repo(repo, branch, prefix)\n",
    "\n",
    "        os.chdir(repo_root)\n",
    "\n",
    "        _install_dependencies_colab()\n",
    "\n",
    "    else: # move to the repo root\n",
    "        os.chdir(repo_dir)\n",
    "\n",
    "def _clone_repo(repo, branch, prefix):\n",
    "    url = f\"https://{prefix}github.com/hmaguire/{repo}\"\n",
    "    subprocess.run(  # run git clone\n",
    "        [\"git\", \"clone\", \"--branch\", branch, \"-q\", url], check=True)\n",
    "\n",
    "def _install_dependencies_colab():\n",
    "    subprocess.run( # directly pip install the prod requirements\n",
    "        [\"pip\", \"install\", \"--quiet\", \"-r\", \"requirements/prod.in\"], check=True)\n",
    "\n",
    "    # run a series of commands with pipes to pip install the dev requirements\n",
    "    subprocess.run(\n",
    "        [\"sed 1d requirements/dev.in | grep -v '#' | xargs pip install --quiet\"],\n",
    "        shell=True, check=True)\n",
    "\n",
    "    # reset pkg_resources list of requirements so gradio can ifner its version correctly\n",
    "    import pkg_resources\n",
    "\n",
    "    pkg_resources._initialize_master_working_set()\n",
    "\n",
    "\n",
    "bootstrap_run = True\n",
    "if \"bootstrap\" not in locals() or bootstrap_run:\n",
    "    # path management for Python\n",
    "    pythonpath, = !echo $PYTHONPATH\n",
    "    if \".\" not in pythonpath.split(\":\"):\n",
    "        pythonpath = \".:\" + pythonpath\n",
    "        %env PYTHONPATH={pythonpath}\n",
    "        !echo $PYTHONPATH\n",
    "\n",
    "    # get both Colab and local notebooks into the same state\n",
    "    _go()\n",
    "\n",
    "    bootstrap = True\n",
    "    # allow \"hot-reloading\" of modules\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "\n",
    "    bootstrap_run = False  # change to True re-run setup\n",
    "\n",
    "!pwd\n",
    "%ls"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w00JmPeXnmWo",
    "outputId": "18a49cb8-4fce-48e7-bc19-f59abb32e685",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONPATH=.:/Users/henry/MoonBoard-Transformer-ML\n",
      ".:/Users/henry/MoonBoard-Transformer-ML\r\n",
      "/Users/henry/MoonBoard-Transformer-ML\r\n",
      "LICENSE.txt      \u001B[34m__pycache__\u001B[m\u001B[m/     \u001B[34mgrade_predictor\u001B[m\u001B[m/ \u001B[34mrequirements\u001B[m\u001B[m/\r\n",
      "Makefile         \u001B[34mdata\u001B[m\u001B[m/            \u001B[34mnotebooks\u001B[m\u001B[m/       \u001B[34mtasks\u001B[m\u001B[m/\r\n",
      "README.md        environment.yml  pyproject.toml   \u001B[34mtraining\u001B[m\u001B[m/\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Download compilation of 2016 moonboard problems data from github project."
   ],
   "metadata": {
    "id": "NEyL1ubV64CT",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from IPython.display import display, HTML, IFrame\n",
    "\n",
    "full_width = True\n",
    "frame_height = 720  # adjust for your screen\n",
    "\n",
    "if full_width:  # if we want the notebook to take up the whole width\n",
    "    # add styling to the notebook's HTML directly\n",
    "    display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "    display(HTML(\"<style>.output_result { max-width:100% !important; }</style>\"))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I1sT8VDbQwW1",
    "outputId": "9de16873-8866-4033-ba1c-482c6074eaeb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>.container { width:100% !important; }</style>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>.output_result { max-width:100% !important; }</style>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a pandas DataFrame from pickle file."
   ],
   "metadata": {
    "id": "KuSPfZrvXsVe",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "import torch\n",
    "\n",
    "\n",
    "gpus = int(torch.cuda.is_available())\n",
    "\n",
    "%run training/run_experiment.py --model_class MB2016Transformer --data_class MB2016 \\\n",
    "  --batch_size 32 --gpus {gpus} --max_epochs 2 \\\n",
    "  --limit_train_batches 0.1 --limit_val_batches 0.1 --limit_test_batches 0.1 --log_every_n_steps 10 --fast_dev_run=True"
   ],
   "metadata": {
    "id": "LYJROLFLqbY5",
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "PYTORCH_ENABLE_MPS_FALLBACK=1\n",
    "\n",
    "%run training/run_experiment.py --model_class MB2016Transformer --lr 0.05 --data_class MB2016 \\\n",
    "  --batch_size 32 --max_epochs 2 \\\n",
    "  --limit_train_batches 0.1 --limit_val_batches 0.1 --limit_test_batches 0.1 --log_every_n_steps 10 --fast_dev_run=True --accelerator=\"mps\""
   ],
   "metadata": {
    "id": "6S_meV27d5Qr",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: run_experiment.py [--logger [LOGGER]]\n",
      "                         [--checkpoint_callback [CHECKPOINT_CALLBACK]]\n",
      "                         [--enable_checkpointing [ENABLE_CHECKPOINTING]]\n",
      "                         [--default_root_dir DEFAULT_ROOT_DIR]\n",
      "                         [--gradient_clip_val GRADIENT_CLIP_VAL]\n",
      "                         [--gradient_clip_algorithm GRADIENT_CLIP_ALGORITHM]\n",
      "                         [--process_position PROCESS_POSITION]\n",
      "                         [--num_nodes NUM_NODES]\n",
      "                         [--num_processes NUM_PROCESSES] [--devices DEVICES]\n",
      "                         [--gpus GPUS] [--auto_select_gpus [AUTO_SELECT_GPUS]]\n",
      "                         [--tpu_cores TPU_CORES] [--ipus IPUS]\n",
      "                         [--log_gpu_memory LOG_GPU_MEMORY]\n",
      "                         [--progress_bar_refresh_rate PROGRESS_BAR_REFRESH_RATE]\n",
      "                         [--enable_progress_bar [ENABLE_PROGRESS_BAR]]\n",
      "                         [--overfit_batches OVERFIT_BATCHES]\n",
      "                         [--track_grad_norm TRACK_GRAD_NORM]\n",
      "                         [--check_val_every_n_epoch CHECK_VAL_EVERY_N_EPOCH]\n",
      "                         [--fast_dev_run [FAST_DEV_RUN]]\n",
      "                         [--accumulate_grad_batches ACCUMULATE_GRAD_BATCHES]\n",
      "                         [--max_epochs MAX_EPOCHS] [--min_epochs MIN_EPOCHS]\n",
      "                         [--max_steps MAX_STEPS] [--min_steps MIN_STEPS]\n",
      "                         [--max_time MAX_TIME]\n",
      "                         [--limit_train_batches LIMIT_TRAIN_BATCHES]\n",
      "                         [--limit_val_batches LIMIT_VAL_BATCHES]\n",
      "                         [--limit_test_batches LIMIT_TEST_BATCHES]\n",
      "                         [--limit_predict_batches LIMIT_PREDICT_BATCHES]\n",
      "                         [--val_check_interval VAL_CHECK_INTERVAL]\n",
      "                         [--flush_logs_every_n_steps FLUSH_LOGS_EVERY_N_STEPS]\n",
      "                         [--log_every_n_steps LOG_EVERY_N_STEPS]\n",
      "                         [--accelerator ACCELERATOR] [--strategy STRATEGY]\n",
      "                         [--sync_batchnorm [SYNC_BATCHNORM]]\n",
      "                         [--precision PRECISION]\n",
      "                         [--enable_model_summary [ENABLE_MODEL_SUMMARY]]\n",
      "                         [--weights_summary WEIGHTS_SUMMARY]\n",
      "                         [--weights_save_path WEIGHTS_SAVE_PATH]\n",
      "                         [--num_sanity_val_steps NUM_SANITY_VAL_STEPS]\n",
      "                         [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]\n",
      "                         [--profiler PROFILER] [--benchmark [BENCHMARK]]\n",
      "                         [--deterministic [DETERMINISTIC]]\n",
      "                         [--reload_dataloaders_every_n_epochs RELOAD_DATALOADERS_EVERY_N_EPOCHS]\n",
      "                         [--auto_lr_find [AUTO_LR_FIND]]\n",
      "                         [--replace_sampler_ddp [REPLACE_SAMPLER_DDP]]\n",
      "                         [--detect_anomaly [DETECT_ANOMALY]]\n",
      "                         [--auto_scale_batch_size [AUTO_SCALE_BATCH_SIZE]]\n",
      "                         [--prepare_data_per_node [PREPARE_DATA_PER_NODE]]\n",
      "                         [--plugins PLUGINS] [--amp_backend AMP_BACKEND]\n",
      "                         [--amp_level AMP_LEVEL]\n",
      "                         [--move_metrics_to_cpu [MOVE_METRICS_TO_CPU]]\n",
      "                         [--multiple_trainloader_mode MULTIPLE_TRAINLOADER_MODE]\n",
      "                         [--stochastic_weight_avg [STOCHASTIC_WEIGHT_AVG]]\n",
      "                         [--terminate_on_nan [TERMINATE_ON_NAN]] [--wandb]\n",
      "                         [--profile] [--data_class DATA_CLASS]\n",
      "                         [--model_class MODEL_CLASS]\n",
      "                         [--load_checkpoint LOAD_CHECKPOINT]\n",
      "                         [--stop_early STOP_EARLY] [--batch_size BATCH_SIZE]\n",
      "                         [--num_workers NUM_WORKERS]\n",
      "                         [--embedding_size EMBEDDING_SIZE]\n",
      "                         [--tf_nheads TF_NHEADS] [--tf_nlayers TF_NLAYERS]\n",
      "                         [--tf_dropout TF_DROPOUT] [--tf_ff_size TF_FF_SIZE]\n",
      "                         [--tf_fc2_size TF_FC2_SIZE]\n",
      "                         [--model_complexity MODEL_COMPLEXITY] [--help]\n",
      "run_experiment.py: error: unrecognized arguments: --lr 0.05\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001B[0;31mSystemExit\u001B[0m\u001B[0;31m:\u001B[0m 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.01 ms, sys: 5.72 ms, total: 9.73 ms\n",
      "Wall time: 16.4 ms\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# from pathlib import Path\n",
    "#\n",
    "# DATA_DIRNAME = Path(__file__).resolve().parents[3] / \"data\"\n",
    "# DOWNLOADED_DATA_DIRNAME = DATA_DIRNAME / \"downloaded\"\n"
   ],
   "metadata": {
    "id": "AddTtClpf-iK",
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "eda2tWwshEuJ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "8VKDWETLfv-L",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "SYMiog7gi6yY",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from torch.utils.data import random_split\n",
    "import itertools\n",
    "from itertools import permutations\n",
    "\n",
    "MINIMUM_REPEATS = 2\n",
    "MINIMUM_GRADE_COUNT = 50\n",
    "MAX_START_HOLDS = 2\n",
    "MAX_MID_HOLDS = 11\n",
    "MIN_MID_HOLDS = 2\n",
    "MAX_END_HOLDS = 2\n",
    "\n",
    "MAX_SEQUENCE = MAX_START_HOLDS + MAX_MID_HOLDS + MAX_END_HOLDS\n",
    "\n",
    "#remove climbs with low or no amounts of repeats\n",
    "cleaned_data = data[data['repeats'] >= MINIMUM_REPEATS]\n",
    "\n",
    "#remove grades that include below a minimum amount of climbs\n",
    "grade_count = cleaned_data['grade'].value_counts()\n",
    "grades = [key for key, value in grade_count.items() if value>= MINIMUM_GRADE_COUNT]\n",
    "cleaned_data = cleaned_data[cleaned_data['grade'].isin(grades)]\n",
    "\n",
    "\n",
    "# remove climbs with extremely low or high number of mid holds used\n",
    "cleaned_data = cleaned_data[cleaned_data.mid.map((len)) <= MAX_MID_HOLDS]\n",
    "cleaned_data = cleaned_data[cleaned_data.mid.map((len)) >= MIN_MID_HOLDS]\n",
    "#remove climbs with more than 2 start holds\n",
    "cleaned_data = cleaned_data[cleaned_data.start.map((len)) <= MAX_START_HOLDS]\n",
    "cleaned_data = cleaned_data[cleaned_data.end.map((len)) <= MAX_END_HOLDS]\n",
    "\n",
    "# Create token dictionary\n",
    "rows = range(0,11)\n",
    "columns = range(0,18)\n",
    "positions = itertools.product(rows, columns)\n",
    "token_dict = {}\n",
    "for i,k in enumerate(positions, start=1):\n",
    "  token_dict[k] = i\n",
    "\n",
    "def df_row_to_input(row, max_sequence):\n",
    "  input = torch.zeros((4, max_sequence), dtype=torch.int)\n",
    "  i = 0\n",
    "  for position_column, position_index in [(row['start'], 1), (row['mid'], 2), (row['end'], 3)]:\n",
    "    for item in position_column:\n",
    "      input[0][i] = token_dict[tuple(item)]\n",
    "      input[1][i] = position_index\n",
    "      input[2][i] = tuple(item)[0]\n",
    "      input[3][i] = tuple(item)[1]\n",
    "      i +=1\n",
    "  return input\n",
    "\n",
    "def df_to_input_array(df):\n",
    "  input_array = torch.zeros(df.shape[0], 4, MAX_SEQUENCE)\n",
    "\n",
    "  input_array = df.apply(lambda x: df_row_to_input(x,MAX_SEQUENCE), axis=1)\n",
    "  return input_array\n",
    "\n",
    "def positional_2d_space_encoding():\n",
    "  return\n",
    "\n",
    "def positional_1d_encoding():\n",
    "  return\n",
    "\n",
    "\n",
    "# Grade stratified test and train split\n",
    "# split = StratifiedShuffleSplit(n_splits=1, test_size = 0.2, random_state=42)\n",
    "# for train_index, test_index in split.split(cleaned_data,cleaned_data['grade']):\n",
    "#   strat_train_set = cleaned_data.iloc[train_index]\n",
    "#   strat_test_set = cleaned_data.iloc[test_index]\n",
    "\n"
   ],
   "metadata": {
    "id": "nvcXwXLuuJaa",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# X Inputs:\n",
    "X = df_to_input_array(cleaned_data.loc[:,['start', 'mid', 'end']])"
   ],
   "metadata": {
    "id": "aUv0bQTfT-D0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Y labels\n",
    "lb = LabelBinarizer()\n",
    "Y = data.loc[:,'grade']\n",
    "Y = torch.FloatTensor(lb.fit_transform(Y))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "id": "FlH_2g7CUGB5",
    "outputId": "ee4a7efb-cc29-4c30-ca41-be6373a9ed86",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# pip install einops\n",
    "# pip install positional-encodings[pytorch]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "Qox0thQP2Hhk",
    "outputId": "3aa44980-d94b-4c1b-ff46-112012c11941",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "\n",
    "# borrowed from lucidrains\n",
    "#https://github.com/lucidrains/bottleneck-transformer-pytorch/blob/main/bottleneck_transformer_pytorch/bottleneck_transformer_pytorch.py#L21\n",
    "def relative_to_absolute(q):\n",
    "    \"\"\"\n",
    "    Converts the dimension that is specified from the axis\n",
    "    from relative distances (with length 2*tokens-1) to absolute distance (length tokens)\n",
    "      Input: [bs, heads, length, 2*length - 1]\n",
    "      Output: [bs, heads, length, length]\n",
    "    \"\"\"\n",
    "    b, h, l, _, device, dtype = *q.shape, q.device, q.dtype\n",
    "    dd = {'device': device, 'dtype': dtype}\n",
    "    col_pad = torch.zeros((b, h, l, 1), **dd)\n",
    "    x = torch.cat((q, col_pad), dim=3)  # zero pad 2l-1 to 2l\n",
    "    flat_x = rearrange(x, 'b h l c -> b h (l c)')\n",
    "    flat_pad = torch.zeros((b, h, l - 1), **dd)\n",
    "    flat_x_padded = torch.cat((flat_x, flat_pad), dim=2)\n",
    "    final_x = flat_x_padded.reshape(b, h, l + 1, 2 * l - 1)\n",
    "    final_x = final_x[:, :, :l, (l - 1):]\n",
    "    return final_x\n",
    "\n",
    "\n",
    "def rel_pos_emb_1d(q, rel_emb, shared_heads):\n",
    "   \"\"\"\n",
    "   Same functionality as RelPosEmb1D\n",
    "\n",
    "   Args:\n",
    "       q: a 4d tensor of shape [batch, heads, tokens, dim]\n",
    "       rel_emb: a 2D or 3D tensor\n",
    "       of shape [ 2*tokens-1 , dim] or [ heads, 2*tokens-1 , dim]\n",
    "   \"\"\"\n",
    "   if shared_heads:\n",
    "       emb = torch.einsum('b h t d, r d -> b h t r', q, rel_emb)\n",
    "   else:\n",
    "       emb = torch.einsum('b h t d, h r d -> b h t r', q, rel_emb)\n",
    "   return relative_to_absolute(emb)\n",
    "\n",
    "class RelPosEmb1D(nn.Module):\n",
    "    def __init__(self, feat_map_size, dim_head, heads=None):\n",
    "      \"\"\"\n",
    "      Outputs\n",
    "      Inputs\n",
    "      \"\"\"\n",
    "\n",
    "      super().__init__()\n",
    "      self.h, self.w = (18,11)  # height, width of Moonboard hold map\n",
    "      self.total_tokens = self.h * self.w\n",
    "      self.shared_heads = heads if heads is not None else True\n",
    "\n",
    "      self.emb_w = rel_pos_emb_1d(self.h, dim_head, heads)\n",
    "      self.emb_h = rel_pos_emb_1d(self.w, dim_head, heads)\n",
    "\n",
    "      scale = dim_head ** -0.5\n",
    "      self.shared_heads = heads if heads is not None else True\n",
    "      if self.shared_heads:\n",
    "        self.rel_pos_emb = nn.Parameter(torch.randn(2 * tokens - 1, dim_head) * scale)\n",
    "      else:\n",
    "        self.rel_pos_emb = nn.Parameter(torch.randn(heads, 2 * tokens - 1, dim_head) * scale)\n",
    "\n",
    "    def forward(self, q, position_info):\n",
    "      return rel_pos_emb_1d(q, self.rel_pos_emb, self.shared_heads)"
   ],
   "metadata": {
    "id": "4EDDAqdL1zyK",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data imput\n",
    "X: Token id list [0,543,432,1,23,24,56,676,3,4,4,4,4]\n",
    "X 2D position\n",
    "X 1D position\n",
    "\n",
    "[S, [4, 7], [1, 10],SM,[4, 12], [6, 12], ME, [6, 16], E, P,P,P]\n",
    "-> token id list [0,45,54,1,63,74,2,82,3,4,4,4] [tokens, dim]\n",
    "-> 1D [0,1,1,2,3,3,4,5,6,6,6] >\n",
    "[tokens, dim]\n",
    "pos_emb1D = torch.nn.Parameter(torch.randn(max_seq_tokens, dim))\n",
    "\n",
    "-> 2D -> yD and xD combined -> [tokens, dim] with exception for S,SM,ME,E,P.\n",
    "x > 11 horizontal > -10 to 10\n",
    "y > 17 vertical > - 16 to 16\n",
    "    \n",
    "Y: Label binarize [0,0,0,1,0,0,0]\n",
    "\n",
    "\n",
    "\n",
    "# initialization\n",
    "pos_emb1D = torch.nn.Parameter(torch.randn(max_seq_tokens, dim))\n",
    "# during forward pass\n",
    "input_to_transformer_mhsa = input_embedding + pos_emb1D[:current_seq_tokens, :]\n",
    "\n",
    "\n",
    "encoding makes this easier\n",
    "\n",
    "[4,7] > token id > token embedding of size dim\n",
    "\n",
    "batchsize, x, channel\n",
    "50 tokens, 1d positions, embeddding size\n",
    "batchsize, x,y, channel\n",
    "\n",
    "\n",
    "input:\n",
    "single data example for d/model size 4\n",
    "max sequence length: 4\n",
    "one word: [0.1,0.5,0.9,0.1,0.2] size 5 token\n",
    "[0] series position -> [0.1,0.4,0.7,0.8,0.9] sin encoding for 0\n",
    "[7,8] x,y abs position -> [0.1,0.4,0.7,0.8,0.9] encoding in 2d space for [7,8]\n",
    "\n",
    "\n",
    "embed:\n",
    "positon\n",
    "x\n",
    "y\n",
    "\n",
    "\n",
    "3d positional encoding dimensions:\n",
    "batchsize, tokens, max_sequence, max_width, max_height, channels\n",
    "\n",
    "\n",
    "reduce to appropirate\n",
    "\n"
   ],
   "metadata": {
    "id": "PTmqmZCihRQL",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import argparse\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from torch.utils.data import random_split\n",
    "import itertools\n",
    "from itertools import permutations\n",
    "\n",
    "from text_recognizer.data.base_data_module import BaseDataModule, load_and_print_info\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# DIMS = (1, 28, 28)\n",
    "# OUTPUT_DIMS = (1,)\n",
    "# MAPPING = list(range(10))\n",
    "# TRAIN_SIZE = 55000\n",
    "# VAL_SIZE = 5000\n",
    "\n",
    "\n",
    "\"\"\" Metada \"\"\"\n",
    "\n",
    "# DL_DATA_DIRNAME = \"/content/drive/MyDrive/Datasets/\"\n",
    "# EXTRACTED_DATASET_DIRNAME = DL_DATA_DIRNAME / \"moonGen_scrape_2016_final.pkl\"\n",
    "\n",
    "MINIMUM_REPEATS = 2\n",
    "MINIMUM_GRADE_COUNT = 50\n",
    "MAX_START_HOLDS = 2\n",
    "MAX_MID_HOLDS = 11\n",
    "MIN_MID_HOLDS = 2\n",
    "MAX_END_HOLDS = 2\n",
    "\n",
    "class MB2016(BaseDataModule):\n",
    "    \"\"\"MB2016 DataModule.\"\"\"\n",
    "\n",
    "    def __init__(self, args=None) -> None:\n",
    "        super().__init__(args)\n",
    "        # self.data_dir = metadata.DOWNLOADED_DATA_DIRNAME\n",
    "        # self.transform = MNISTStem()\n",
    "        # self.input_dims = metadata.DIMS\n",
    "        # self.output_dims = OUTPUT_DIMS\n",
    "        # self.mapping = metadata.MAPPING\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "\n",
    "        #extract dataset to pandas dataframe\n",
    "        pkl_file = open('/content/drive/MyDrive/Datasets/moonGen_scrape_2016_final.pkl', 'rb')\n",
    "        pickle_data = pickle.load(pkl_file)\n",
    "        pkl_file.close()\n",
    "        data = pd.DataFrame.from_dict(pickle_data).T\n",
    "\n",
    "        #remove climbs with low or no amounts of repeats\n",
    "        cleaned_data = data[data['repeats'] >= MINIMUM_REPEATS]\n",
    "\n",
    "        #remove climbs with grades that contain less than a minimum number of climbs\n",
    "        grade_count = cleaned_data['grade'].value_counts()\n",
    "        grades = [key for key, value in grade_count.items() if value>= MINIMUM_GRADE_COUNT]\n",
    "        cleaned_data = cleaned_data[cleaned_data['grade'].isin(grades)]\n",
    "\n",
    "        # max length >>> TODO Clean remove climbs with crazy number of holds\n",
    "        # add <S> <SM> <ME> <P> <P> <P> for x input\n",
    "        # mapping for [1,2] etc required > transform to tensor wigth mapped values\n",
    "        # for\n",
    "\n",
    "    def setup(self) -> None:\n",
    "        # Cross validation TODO\n",
    "\n",
    "        # Grade stratified test and train split\n",
    "        split = StratifiedShuffleSplit(n_splits=1, test_size = 0.2, random_state=42)\n",
    "        for train_index, test_index in split.split(cleaned_data,cleaned_data['grade']):\n",
    "          strat_train_set = cleaned_data.iloc[train_index]\n",
    "          strat_test_set = cleaned_data.iloc[test_index]\n",
    "\n",
    "        \"\"\"\n",
    "        pipeline for X input\n",
    "        - weights\n",
    "        - list of lists > list of tokens (use tuples?)\n",
    "        - token has an embedding mapped to it\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        # Token dictinary\n",
    "        # drop repeats with only 1 repeat, and v12 +\n",
    "\n",
    "\n",
    "        X = data.loc[:,['start', 'mid', 'end']]\n",
    "\n",
    "\n",
    "        X['start'] = X['start'].map(lambda x: [ str(x1) for x1 in x])\n",
    "\n",
    "        # X['start'] = X['start'].map(lambda x: np.asarray(x))\n",
    "        # X['mid'] = X['mid'].map(lambda x: np.asarray(x))\n",
    "        # X['end'] = X['end'].map(lambda x: np.asarray(x))\n",
    "\n",
    "        display(X.start[0])\n",
    "        display(X)\n",
    "\n",
    "        X.info(verbose=True)\n",
    "\n",
    "\n",
    "        X['mid'] = X['mid'].apply(lambda x: np.ravel(x))\n",
    "        X['end'] = X['end'].apply(lambda x: np.ravel(x))\n",
    "\n",
    "        Y = data.loc[:,'grade']\n",
    "        lb = LabelBinarizer()\n",
    "        Y = torch.Tensor(lb.fit_transform(Y))\n",
    "\n",
    "\n",
    "data_holds = MB2016()\n",
    "\n",
    "data_holds.prepare_data()"
   ],
   "metadata": {
    "id": "xljspFReA2Jc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch import nn\n",
    "\n",
    "class GradePredictorModel(nn.Module):\n",
    "  def __init__(self) -> None:\n",
    "    super().__init__()\n",
    "    self.input_dims = 5#?\n",
    "    self.num_classes = 7#?\n",
    "    self.dim = 2 #?\n",
    "\n",
    "\n",
    "    self.embedding = nn.Embedding(self.num_classes, self.dim)\n",
    "    self.enc_pos_encoder = Positional2DEncoding()\n",
    "    self.transformer_decoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=self.dim, nhead=tf_nhead, dim_feedforward=tf_fc_dim, dropout=tf_dropout),\n",
    "            num_layers=tf_layers,\n",
    "        )\n",
    "\n",
    "\n",
    "class GradePredictor(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()  # just like in torch.nn.Module, we need to call the parent class __init__\n",
    "\n",
    "        # attach torch.nn.Modules as top level attributes during init, just like in a torch.nn.Module\n",
    "        self.model = torch.nn.Linear(in_features=1, out_features=1)\n",
    "        # we like to define the entire model as one torch.nn.Module -- typically in a separate class\n",
    "\n",
    "    # optionally, define a forward method\n",
    "    def forward(self, xs):\n",
    "        return self.model(xs)  # we like to just call the model's forward method"
   ],
   "metadata": {
    "id": "3WF332YHnMox",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}