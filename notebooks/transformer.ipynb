{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# DIMS = (1, 28, 28)\n",
    "# OUTPUT_DIMS = (1,)\n",
    "# MAPPING = list(range(10))\n",
    "# TRAIN_SIZE = 55000\n",
    "# VAL_SIZE = 5000\n",
    "\n",
    "class MB2016(BaseDataModule):\n",
    "    \"\"\"MB2016 DataModule.\"\"\"\n",
    "\n",
    "    def __init__(self, args=None) -> None:\n",
    "        super().__init__(args)\n",
    "\n",
    "        # self.data_dir = metadata.DOWNLOADED_DATA_DIRNAME\n",
    "        # self.transform = MNISTStem()\n",
    "        # self.input_dims = metadata.DIMS\n",
    "        # self.output_dims = OUTPUT_DIMS\n",
    "        # self.mapping = metadata.MAPPING\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "\n",
    "        # check if data has already been prepared\n",
    "        if self.data_filename.exists():\n",
    "            return\n",
    "\n",
    "        #extract dataset to pandas dataframe\n",
    "        with temporary_working_directory(RAW_DATA_DIRNAME):\n",
    "            with open(self.data_filename, 'rb') as pkl_file:\n",
    "                pickle_data = pickle.load(pkl_file)\n",
    "                data = pd.DataFrame.from_dict(pickle_data).T\n",
    "\n",
    "        #remove climbs with low or no amounts of repeats\n",
    "        cleaned_data = data[data['repeats'] >= MIN_REPEATS]\n",
    "\n",
    "        #remove climbs with grades that contain less than a minimum number of climbs\n",
    "        grade_count = cleaned_data['grade'].value_counts()\n",
    "        grades = [key for key, value in grade_count.items() if value>= MIN_GRADE_COUNT]\n",
    "        cleaned_data = cleaned_data[cleaned_data['grade'].isin(grades)]\n",
    "\n",
    "        # max length >>> TODO Clean remove climbs with crazy number of holds\n",
    "        # add <S> <SM> <ME> <P> <P> <P> for x input\n",
    "        # mapping for [1,2] etc required > transform to tensor wigth mapped values\n",
    "        # for\n",
    "\n",
    "    def setup(self) -> None:\n",
    "        # Cross validation TODO\n",
    "\n",
    "        # Grade stratified test and train split\n",
    "        split = StratifiedShuffleSplit(n_splits=1, test_size = 0.2, random_state=42)\n",
    "        for train_index, test_index in split.split(cleaned_data,cleaned_data['grade']):\n",
    "          strat_train_set = cleaned_data.iloc[train_index]\n",
    "          strat_test_set = cleaned_data.iloc[test_index]\n",
    "\n",
    "        \"\"\"\n",
    "        pipeline for X input\n",
    "        - weights\n",
    "        - list of lists > list of tokens (use tuples?)\n",
    "        - token has an embedding mapped to it\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        # Token dictinary\n",
    "        # drop repeats with only 1 repeat, and v12 +\n",
    "\n",
    "\n",
    "        X = data.loc[:,['start', 'mid', 'end']]\n",
    "\n",
    "\n",
    "        X['start'] = X['start'].map(lambda x: [ str(x1) for x1 in x])\n",
    "\n",
    "        # X['start'] = X['start'].map(lambda x: np.asarray(x))\n",
    "        # X['mid'] = X['mid'].map(lambda x: np.asarray(x))\n",
    "        # X['end'] = X['end'].map(lambda x: np.asarray(x))\n",
    "\n",
    "        display(X.start[0])\n",
    "        display(X)\n",
    "\n",
    "        X.info(verbose=True)\n",
    "\n",
    "\n",
    "        X['mid'] = X['mid'].apply(lambda x: np.ravel(x))\n",
    "        X['end'] = X['end'].apply(lambda x: np.ravel(x))\n",
    "\n",
    "        Y = data.loc[:,'grade']\n",
    "        lb = LabelBinarizer()\n",
    "        Y = torch.Tensor(lb.fit_transform(Y))\n",
    "\n",
    "    @property\n",
    "    def data_filename(self):\n",
    "        return (\n",
    "            PROCESSED_DATA_DIRNAME /\n",
    "            f\"minr_{self.min_repeats}_mingc{self.min_grade_count:f}_maxs{self.max_start_holds:f}\"\n",
    "            f\"minm_{self.min_mid_holds}_maxm{self.max_mid_holds:f}_maxe{self.max_end_holds:f}\"\n",
    "            f\"_ntr{self.num_train}_ntv{self.num_val}_nte{self.num_test}_{self.with_start_mid_end_tokens}.csv\"\n",
    "        )\n",
    "\n",
    "\n",
    "data_holds = MB2016()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from torch.utils.data import random_split\n",
    "import itertools\n",
    "from itertools import permutations\n",
    "\n",
    "from grade_predictor.data.base_data_module import BaseDataModule, load_and_print_info\n",
    "from grade_predictor.util import temporary_working_directory\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import grade_predictor.metadata.transformer as metadata\n",
    "\n",
    "\n",
    "\"\"\" Metada \"\"\"\n",
    "\n",
    "RAW_DATA_DIRNAME = metadata.RAW_DATA_DIRNAME\n",
    "RAW_DATA_FILENAME = metadata.RAW_DATA_FILENAME\n",
    "DATA_DIRNAME = metadata.DATA_DIRNAME\n",
    "PROCESSED_DATA_DIRNAME = metadata.PROCESSED_DATA_DIRNAME\n",
    "\n",
    "NUM_SPECIAL_TOKENS = 0\n",
    "\n",
    "MIN_REPEATS = 2\n",
    "MIN_GRADE_COUNT = 50\n",
    "MAX_START_HOLDS = 2\n",
    "MIN_MID_HOLDS = 2\n",
    "MAX_MID_HOLDS = 11\n",
    "MAX_END_HOLDS = 2\n",
    "MAX_SEQUENCE = MAX_START_HOLDS + MAX_MID_HOLDS + MAX_END_HOLDS\n",
    "NUM_TRAIN = 100\n",
    "NUM_VAL = 20\n",
    "NUM_TEST = 10\n",
    "\n",
    "\n",
    "class MB2016(BaseDataModule):\n",
    "    \"\"\"MB2016 dataset: Scraped Moonboard 2016 climbs information\"\"\"\n",
    "\n",
    "    def __init__(self, args=None) -> None:\n",
    "\n",
    "        super().__init__(args)\n",
    "\n",
    "        self.min_repeats = self.args.get(\"min_repeats\", MIN_REPEATS)\n",
    "        self.min_grade_count = self.args.get(\"min_grade_count\", MIN_GRADE_COUNT)\n",
    "        self.max_start_holds = self.args.get(\"max_start_holds\", MAX_START_HOLDS)\n",
    "        self.min_mid_holds = self.args.get(\"min_mid_holds\", MIN_MID_HOLDS)\n",
    "        self.max_mid_holds = self.args.get(\"max_mid_holds\", MAX_MID_HOLDS)\n",
    "        self.max_end_holds = self.args.get(\"max_end_holds\", MAX_END_HOLDS)\n",
    "\n",
    "        self.with_start_mid_end_tokens = self.args.get(\"with_start_mid_end_tokens\", False)\n",
    "\n",
    "        self.num_train = self.args.get(\"num_train\", NUM_TRAIN)\n",
    "        self.num_val = self.args.get(\"num_val\", NUM_VAL)\n",
    "        self.num_test = self.args.get(\"num_test\", NUM_TEST)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        self.data_dir = self.args.get(\"data_dir\", DATA_DIRNAME)\n",
    "        self.processed_data_dir = self.args.get(\"data_dir\", PROCESSED_DATA_DIRNAME)\n",
    "        # self.transform = MNISTStem()\n",
    "        # self.input_dims = ()\n",
    "        # self.output_dims = OUTPUT_DIMS\n",
    "        # self.mapping = metadata.MAPPING\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "\n",
    "        # check if data has already been prepared\n",
    "        if self.data_filename.exists():\n",
    "            return\n",
    "\n",
    "        #extract dataset to pandas dataframe\n",
    "        with temporary_working_directory(self.data_dir):\n",
    "            with open(self.data_filename, 'rb') as pkl_file:\n",
    "                pickle_data = pickle.load(pkl_file)\n",
    "                data = pd.DataFrame.from_dict(pickle_data).T\n",
    "\n",
    "        #Clean dataset of extraneous climbs\n",
    "        cleaned_data = data[data['repeats'] >= MIN_REPEATS]\n",
    "        cleaned_data = cleaned_data[cleaned_data.mid.map(len) <= MAX_MID_HOLDS]\n",
    "        cleaned_data = cleaned_data[cleaned_data.mid.map(len) >= MIN_MID_HOLDS]\n",
    "        cleaned_data = cleaned_data[cleaned_data.start.map(len) <= MAX_START_HOLDS]\n",
    "        cleaned_data = cleaned_data[cleaned_data.end.map(len) <= MAX_END_HOLDS]\n",
    "\n",
    "        grade_count = cleaned_data['grade'].value_counts()\n",
    "        grades = [key for key, value in grade_count.items() if value>= MIN_GRADE_COUNT]\n",
    "        cleaned_data = cleaned_data[cleaned_data['grade'].isin(grades)]\n",
    "        #Save cleaned data\n",
    "        with temporary_working_directory(self.processed_data_dir):\n",
    "            cleaned_data.to_pickle(self.data_filename)\n",
    "\n",
    "\n",
    "\n",
    "        # Create token dictionary for MoonBoard hold positions\n",
    "        rows = range(0,11)\n",
    "        columns = range(0,18)\n",
    "        positions = itertools.product(rows, columns)\n",
    "        token_dict = {}\n",
    "        if self.with_start_mid_end_tokens:\n",
    "            # TODO: add add <S> <SM> <ME> <P>\n",
    "            pass\n",
    "        for i,k in enumerate(positions, start=1):\n",
    "            token_dict[k] = i\n",
    "\n",
    "        self.X = _dataframe_to_token_array(cleaned_data.loc[:,['start', 'mid', 'end']])\n",
    "        lb = LabelBinarizer()\n",
    "        self.Y = data.loc[:,'grade']\n",
    "        self.Y = torch.FloatTensor(lb.fit_transform(self.Y))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def setup(self) -> None:\n",
    "        # Cross validation TODO\n",
    "\n",
    "        # Grade stratified test and train split\n",
    "        split = StratifiedShuffleSplit(n_splits=1, test_size = 0.2, random_state=42)\n",
    "        for train_index, test_index in split.split(cleaned_data,cleaned_data['grade']):\n",
    "            strat_train_set = cleaned_data.iloc[train_index]\n",
    "            strat_test_set = cleaned_data.iloc[test_index]\n",
    "\n",
    "    @property\n",
    "    def data_filename(self):\n",
    "        return (\n",
    "            PROCESSED_DATA_DIRNAME /\n",
    "            f\"minr_{self.min_repeats}_mingc{self.min_grade_count:f}_maxs{self.max_start_holds:f}\"\n",
    "            f\"minm_{self.min_mid_holds}_maxm{self.max_mid_holds:f}_maxe{self.max_end_holds:f}\"\n",
    "            f\"_ntr{self.num_train}_ntv{self.num_val}_nte{self.num_test}_{self.with_start_mid_end_tokens}.pkl\"\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "def _dataframe_to_token_array(df, max_sequence, token_dict):\n",
    "    # token_array = torch.zeros(df.shape[0], 4, MAX_SEQUENCE)\n",
    "    def _row_to_token_matrix(row):\n",
    "        token_matrix = torch.zeros((4, max_sequence), dtype=torch.int)\n",
    "        i = 0\n",
    "        for position_column, position_index in [(row['start'], 1), (row['mid'], 2), (row['end'], 3)]:\n",
    "            for item in position_column:\n",
    "                token_matrix[0][i] = token_dict[tuple(item)]\n",
    "                token_matrix[1][i] = position_index\n",
    "                token_matrix[2][i] = tuple(item)[0]\n",
    "                token_matrix[3][i] = tuple(item)[1]\n",
    "                i +=1\n",
    "        return token_matrix\n",
    "\n",
    "    token_array = df.apply(lambda x: _row_to_token_matrix(x), axis=1)\n",
    "    return token_array\n",
    "\n",
    "\n",
    "# def _download_and_process_MB2016():\n",
    "#     metadata = toml.load(METADATA_FILENAME)\n",
    "#     _download_raw_dataset(metadata, DL_DATA_DIRNAME)\n",
    "#     _process_raw_dataset(metadata[\"filename\"], DL_DATA_DIRNAME)\n",
    "\n",
    "def _sample_to_balance(x, y):\n",
    "    \"\"\"Because the dataset is not balanced, we take at most the mean number of instances per class.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    num_to_sample = int(np.bincount(y.flatten()).mean())\n",
    "    all_sampled_inds = []\n",
    "    for label in np.unique(y.flatten()):\n",
    "        inds = np.where(y == label)[0]\n",
    "        sampled_inds = np.unique(np.random.choice(inds, num_to_sample))\n",
    "        all_sampled_inds.append(sampled_inds)\n",
    "    ind = np.concatenate(all_sampled_inds)\n",
    "    x_sampled = x[ind]\n",
    "    y_sampled = y[ind]\n",
    "    return x_sampled, y_sampled\n",
    "\n",
    "load_and_print_info(MB2016)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}