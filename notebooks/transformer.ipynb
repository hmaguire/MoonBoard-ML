{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "A Jupyther notebook is used to rapidly iterate through the development of data ingestion and pre-processing, transformer model architecture and hyperparameter tuning.\n",
    "\n",
    "The MB2016 Data Module processes the raw 2016 Moonboard dataset into train, validation and test datasets ready as inputs for the transformer deep learning model.\n",
    "\n",
    "Debug MB2016 prepare_data and setup."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONPATH=.:/Users/henry/MoonBoard-Transformer-ML\n",
      ".:/Users/henry/MoonBoard-Transformer-ML\r\n",
      "/Users/henry/MoonBoard-Transformer-ML\r\n",
      "LICENSE.txt      \u001B[34m__pycache__\u001B[m\u001B[m/     \u001B[34mgrade_predictor\u001B[m\u001B[m/ \u001B[34mrequirements\u001B[m\u001B[m/\r\n",
      "Makefile         \u001B[34mdata\u001B[m\u001B[m/            \u001B[34mnotebooks\u001B[m\u001B[m/       \u001B[34mtasks\u001B[m\u001B[m/\r\n",
      "README.md        environment.yml  pyproject.toml   \u001B[34mtraining\u001B[m\u001B[m/\r\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Sets up both local Jupyter and Google Colab notebooks in the same state.\"\"\"\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import sys\n",
    "import subprocess\n",
    "from subprocess import PIPE, Popen\n",
    "\n",
    "try:  # check if we're in a git repo\n",
    "    repo_dir = subprocess.run([\"git\", \"rev-parse\", \"--show-toplevel\"], capture_output=True, check=True).stdout.decode().strip()\n",
    "    repo = Path(repo_dir).name\n",
    "except subprocess.CalledProcessError:\n",
    "    repo = os.environ.get(\"MBML_REPO\", \"moonboard-transformer-ml\")\n",
    "\n",
    "branch = os.environ.get(\"MBML_BRANCH\", \"main\")\n",
    "token = os.environ.get(\"MBML_GHTOKEN\")\n",
    "prefix = token + \"@\" if token else \"\"\n",
    "\n",
    "in_colab = \"google.colab\" in sys.modules\n",
    "\n",
    "def _go():\n",
    "    if in_colab: # create the repo and cd into it\n",
    "        repo_root = Path(\"/\") / \"content\" / repo\n",
    "        os.chdir(repo_root.parent)\n",
    "\n",
    "        shutil.rmtree(repo_root, ignore_errors=True)\n",
    "        _clone_repo(repo, branch, prefix)\n",
    "\n",
    "        os.chdir(repo_root)\n",
    "\n",
    "        _install_dependencies_colab()\n",
    "\n",
    "    else: # move to the repo root\n",
    "        os.chdir(repo_dir)\n",
    "\n",
    "def _clone_repo(repo, branch, prefix):\n",
    "    url = f\"https://{prefix}github.com/hmaguire/{repo}\"\n",
    "    subprocess.run(  # run git clone\n",
    "        [\"git\", \"clone\", \"--branch\", branch, \"-q\", url], check=True)\n",
    "\n",
    "def _install_dependencies_colab():\n",
    "    subprocess.run( # directly pip install the prod requirements\n",
    "        [\"pip\", \"install\", \"--quiet\", \"-r\", \"requirements/prod.in\"], check=True)\n",
    "\n",
    "    # run a series of commands with pipes to pip install the dev requirements\n",
    "    subprocess.run(\n",
    "        [\"sed 1d requirements/dev.in | grep -v '#' | xargs pip install --quiet\"],\n",
    "        shell=True, check=True)\n",
    "\n",
    "    # reset pkg_resources list of requirements so gradio can ifner its version correctly\n",
    "    import pkg_resources\n",
    "\n",
    "    pkg_resources._initialize_master_working_set()\n",
    "\n",
    "\n",
    "if \"bootstrap\" not in locals() or bootstrap_run:\n",
    "    # path management for Python\n",
    "    pythonpath, = !echo $PYTHONPATH\n",
    "    if \".\" not in pythonpath.split(\":\"):\n",
    "        pythonpath = \".:\" + pythonpath\n",
    "        %env PYTHONPATH={pythonpath}\n",
    "        !echo $PYTHONPATH\n",
    "\n",
    "    # get both Colab and local notebooks into the same state\n",
    "    _go()\n",
    "\n",
    "    bootstrap = True\n",
    "    # allow \"hot-reloading\" of modules\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "\n",
    "    bootstrap_run = False  # change to True re-run setup\n",
    "\n",
    "!pwd\n",
    "%ls\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing logger folder: training/logs/lightning_logs\r\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\r\n",
      "GPU available: False, used: False\r\n",
      "TPU available: False, using: 0 TPU cores\r\n",
      "IPU available: False, using: 0 IPUs\r\n",
      "HPU available: False, using: 0 HPUs\r\n",
      "Running in fast_dev_run mode: will run a full train, val, test and prediction loop using 1 batch(es).\r\n",
      "`Trainer(limit_train_batches=1)` was configured so 1 batch per epoch will be used.\r\n",
      "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\r\n",
      "`Trainer(limit_test_batches=1)` was configured so 1 batch will be used.\r\n",
      "`Trainer(limit_predict_batches=1)` was configured so 1 batch will be used.\r\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\r\n",
      "\r\n",
      "   | Name                      | Type               | Params\r\n",
      "------------------------------------------------------------------\r\n",
      "0  | model                     | MB2016Transformer  | 661 K \r\n",
      "1  | model.loss_fn             | MSELoss            | 0     \r\n",
      "2  | model.embedding           | Embedding          | 10.0 K\r\n",
      "3  | model.transformer_encoder | TransformerEncoder | 651 K \r\n",
      "4  | model.linear              | Linear             | 51    \r\n",
      "5  | model.flatten             | Flatten            | 0     \r\n",
      "6  | model.linear2             | Linear             | 16    \r\n",
      "7  | train_mae                 | MeanAbsoluteError  | 0     \r\n",
      "8  | val_mae                   | MeanAbsoluteError  | 0     \r\n",
      "9  | test_mae                  | MeanAbsoluteError  | 0     \r\n",
      "10 | train_mse                 | MeanSquaredError   | 0     \r\n",
      "11 | val_mse                   | MeanSquaredError   | 0     \r\n",
      "12 | test_mse                  | MeanSquaredError   | 0     \r\n",
      "------------------------------------------------------------------\r\n",
      "661 K     Trainable params\r\n",
      "0         Non-trainable params\r\n",
      "661 K     Total params\r\n",
      "2.648     Total estimated model params size (MB)\r\n",
      "Model State Dict Disk Size: 2.66 MB\r\n",
      "/Users/henry/anaconda3/envs/moonboard-transformer-ml/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1927: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\r\n",
      "  rank_zero_warn(\r\n",
      "Epoch 0:   0%|                                            | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "!python training/run_experiment.py --max_epochs=10 --accelerator=cpu --model_class=MB2016Transformer --data_class=MB2016 --fast_dev_run=True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}